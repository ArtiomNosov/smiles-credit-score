{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71832bc8-90e2-4938-8ae6-43ad27a3fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d221b27c-b2bf-4edb-bc02-4bb6539123a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA_PATH = \"/trinity/home/team08/workspace/data/train_data/\"\n",
    "# TEST_DATA_PATH = \"/trinity/home/team08/workspace/data/test_data/\"\n",
    "\n",
    "# TRAIN_TARGET_PATH = \"/trinity/home/team08/workspace/data/train_target.csv\"\n",
    "# TEST_TARGET_PATH = \"/trinity/home/team08/workspace/data/test_target.csv\"\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = \"/trinity/home/team08/workspace/main_project/train_data/\"\n",
    "TEST_DATA_PATH = \"/trinity/home/team08/workspace/main_project/test_data/\"\n",
    "\n",
    "TRAIN_TARGET_PATH = \"/trinity/home/team08/workspace/data/train_target.csv\"\n",
    "TEST_TARGET_PATH = \"/trinity/home/team08/workspace/data/test_target.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf9e519-c969-4572-a35a-a8df6de5f31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20827</th>\n",
       "      <td>20827</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20828</th>\n",
       "      <td>20828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20829</th>\n",
       "      <td>20829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20830</th>\n",
       "      <td>20830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20831</th>\n",
       "      <td>20831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20832 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  flag\n",
       "0          0     0\n",
       "1          1     0\n",
       "2          2     0\n",
       "3          3     0\n",
       "4          4     0\n",
       "...      ...   ...\n",
       "20827  20827     0\n",
       "20828  20828     0\n",
       "20829  20829     0\n",
       "20830  20830     0\n",
       "20831  20831     0\n",
       "\n",
       "[20832 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target = pd.read_csv(TRAIN_TARGET_PATH)\n",
    "train_target = train_target.iloc[:20832]\n",
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411554c7-5f98-4bd0-ba59-72482f8cac2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_target[\u001b[43mdata_frame\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_frame' is not defined"
     ]
    }
   ],
   "source": [
    "train_target[data_frame['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84b778-5d8c-408d-a8ae-fb5353260dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка Parquet файла\n",
    "file_path = '/trinity/home/team08/workspace/data/train_data/train_data_0.pq'\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Группировка данных по пользователю\n",
    "user_groups = df.groupby('id')\n",
    "\n",
    "# Создание списка групп\n",
    "grouped_data = [group for _, group in user_groups]\n",
    "\n",
    "# Определение размера каждой части\n",
    "total_groups = len(grouped_data)\n",
    "part_size = total_groups // 12\n",
    "\n",
    "# Разделение на 12 частей, избегая разбиения истории одного пользователя\n",
    "parts = []\n",
    "for i in range(12):\n",
    "    start_index = i * part_size\n",
    "    end_index = (i + 1) * part_size if i < 11 else total_groups\n",
    "    part = pd.concat(grouped_data[start_index:end_index])\n",
    "    parts.append(part)\n",
    "\n",
    "output_dir = '/trinity/home/team08/workspace/main_project/train_data'\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "# Сохранение каждой части в отдельный Parquet файл\n",
    "for i, part in enumerate(parts):\n",
    "    part_file_path = os.path.join(output_dir, f'part_{i+1}.pqt')\n",
    "    part.to_parquet(part_file_path)\n",
    "\n",
    "print(\"Разделение завершено. Файлы сохранены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3144bb-22ff-47bf-83bc-06d251ca9ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разделение завершено. Файлы сохранены.\n"
     ]
    }
   ],
   "source": [
    "# Загрузка Parquet файла\n",
    "file_path = '/trinity/home/team08/workspace/data/test_data/test_data_0.pq'\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Группировка данных по пользователю\n",
    "user_groups = df.groupby('id')\n",
    "\n",
    "# Создание списка групп\n",
    "grouped_data = [group for _, group in user_groups]\n",
    "\n",
    "# Определение размера каждой части\n",
    "total_groups = len(grouped_data)\n",
    "part_size = total_groups // 2\n",
    "\n",
    "# Разделение на 2 части, избегая разбиения истории одного пользователя\n",
    "parts = []\n",
    "for i in range(2):\n",
    "    start_index = i * part_size\n",
    "    end_index = (i + 1) * part_size if i < 1 else total_groups\n",
    "    part = pd.concat(grouped_data[start_index:end_index])\n",
    "    parts.append(part)\n",
    "\n",
    "output_dir = '/trinity/home/team08/workspace/main_project/test_data'\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "# Сохранение каждой части в отдельный Parquet файл\n",
    "for i, part in enumerate(parts):\n",
    "    part_file_path = os.path.join(output_dir, f'part_{i+1}.pqt')\n",
    "    part.to_parquet(part_file_path)\n",
    "\n",
    "print(\"Разделение завершено. Файлы сохранены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129ddd30-1827-4c66-bba9-456e08e99c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0, num_parts_to_read: int = 2,\n",
    "                                    columns: List[str] = None, verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Читает ``num_parts_to_read`` партиций и преобразует их к pandas.DataFrame.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    path_to_dataset: str\n",
    "        Путь до директории с партициями.\n",
    "    start_from: int, default=0\n",
    "        Номер партиции, с которой начать чтение.\n",
    "    num_parts_to_read: int, default=2\n",
    "        Число партиций, которые требуется прочитать.\n",
    "    columns: List[str], default=None\n",
    "        Список колонок, которые нужно прочитать из каждой партиции. Если None, то считываются все колонки.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    frame: pandas.DataFrame\n",
    "        Прочитанные партиции, преобразованные к pandas.DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    res = []\n",
    "    start_from = max(0, start_from)\n",
    "    # dictionory of format {partition number: partition filename}\n",
    "    dataset_paths = {int(os.path.splitext(filename)[0].split(\"_\")[-1]): os.path.join(path_to_dataset, filename)\n",
    "                     for filename in os.listdir(path_to_dataset)}\n",
    "\n",
    "\n",
    "    chunks = [dataset_paths[num] for num in sorted(dataset_paths.keys()) if num>=start_from][:num_parts_to_read]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Reading chunks:\", *chunks, sep=\"\\n\")\n",
    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        chunk = pd.read_parquet(chunk_path, columns=columns)\n",
    "        res.append(chunk)\n",
    "    return pd.concat(res).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adec2fa7-bfa9-404d-a0aa-7d52a5b8dbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2412745/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12363ab836c44188eed7f3930dde0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_frame = read_parquet_dataset_from_local(TRAIN_DATA_PATH, start_from=0, num_parts_to_read=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886b88ad-4279-46f3-a623-e74a6ac842de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rn</th>\n",
       "      <th>pre_since_opened</th>\n",
       "      <th>pre_since_confirmed</th>\n",
       "      <th>pre_pterm</th>\n",
       "      <th>pre_fterm</th>\n",
       "      <th>pre_till_pclose</th>\n",
       "      <th>pre_till_fclose</th>\n",
       "      <th>pre_loans_credit_limit</th>\n",
       "      <th>pre_loans_next_pay_summ</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_paym_21</th>\n",
       "      <th>enc_paym_22</th>\n",
       "      <th>enc_paym_23</th>\n",
       "      <th>enc_paym_24</th>\n",
       "      <th>enc_loans_account_holder_type</th>\n",
       "      <th>enc_loans_credit_status</th>\n",
       "      <th>enc_loans_credit_type</th>\n",
       "      <th>enc_loans_account_cur</th>\n",
       "      <th>pclose_flag</th>\n",
       "      <th>fclose_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  rn  pre_since_opened  pre_since_confirmed  pre_pterm  pre_fterm  \\\n",
       "0    0   1                18                    9          2          3   \n",
       "1    0   2                18                    9         14         14   \n",
       "2    0   3                18                    9          4          8   \n",
       "3    0   4                 4                    1          9         12   \n",
       "4    0   5                 5                   12         15          2   \n",
       "5    0   6                 5                    0         11          8   \n",
       "6    0   7                 3                    9          1          2   \n",
       "7    0   8                 2                    9          2          3   \n",
       "8    0   9                 1                    9         11         13   \n",
       "9    0  10                 7                    9          2         10   \n",
       "10   1   1                 8                    7         12         15   \n",
       "11   1   2                 8                    7         13          2   \n",
       "12   1   3                15                    7          7          6   \n",
       "13   1   4                15                    3          7          6   \n",
       "14   1   5                14                    4          7          6   \n",
       "\n",
       "    pre_till_pclose  pre_till_fclose  pre_loans_credit_limit  \\\n",
       "0                16               10                      11   \n",
       "1                12               12                       0   \n",
       "2                 1               11                      11   \n",
       "3                16                7                      12   \n",
       "4                11               12                      10   \n",
       "5                12               11                       4   \n",
       "6                12               14                      15   \n",
       "7                12               14                      15   \n",
       "8                14                8                       2   \n",
       "9                 8                8                      16   \n",
       "10                9                1                       1   \n",
       "11                9                1                       6   \n",
       "12                9                1                       6   \n",
       "13                9                1                       6   \n",
       "14                9                1                       3   \n",
       "\n",
       "    pre_loans_next_pay_summ  ...  enc_paym_21  enc_paym_22  enc_paym_23  \\\n",
       "0                         3  ...            3            3            3   \n",
       "1                         3  ...            0            0            0   \n",
       "2                         0  ...            0            0            0   \n",
       "3                         2  ...            3            3            3   \n",
       "4                         2  ...            3            3            3   \n",
       "5                         2  ...            3            3            3   \n",
       "6                         5  ...            3            3            3   \n",
       "7                         5  ...            3            3            3   \n",
       "8                         5  ...            3            3            3   \n",
       "9                         4  ...            3            3            3   \n",
       "10                        2  ...            0            3            3   \n",
       "11                        2  ...            3            3            3   \n",
       "12                        2  ...            3            3            3   \n",
       "13                        2  ...            3            3            3   \n",
       "14                        2  ...            0            1            0   \n",
       "\n",
       "    enc_paym_24  enc_loans_account_holder_type  enc_loans_credit_status  \\\n",
       "0             4                              1                        3   \n",
       "1             4                              1                        3   \n",
       "2             4                              1                        2   \n",
       "3             4                              1                        3   \n",
       "4             4                              1                        3   \n",
       "5             4                              1                        2   \n",
       "6             4                              1                        3   \n",
       "7             4                              1                        3   \n",
       "8             4                              1                        2   \n",
       "9             4                              1                        2   \n",
       "10            4                              1                        3   \n",
       "11            4                              1                        3   \n",
       "12            4                              1                        3   \n",
       "13            4                              1                        3   \n",
       "14            1                              1                        3   \n",
       "\n",
       "    enc_loans_credit_type  enc_loans_account_cur  pclose_flag  fclose_flag  \n",
       "0                       4                      1            0            0  \n",
       "1                       4                      1            0            0  \n",
       "2                       3                      1            1            1  \n",
       "3                       1                      1            0            0  \n",
       "4                       4                      1            0            0  \n",
       "5                       3                      1            0            1  \n",
       "6                       4                      1            0            0  \n",
       "7                       4                      1            0            0  \n",
       "8                       4                      1            0            0  \n",
       "9                       4                      1            0            0  \n",
       "10                      4                      1            0            0  \n",
       "11                      4                      1            0            0  \n",
       "12                      4                      1            0            0  \n",
       "13                      1                      1            0            0  \n",
       "14                      4                      1            0            0  \n",
       "\n",
       "[15 rows x 61 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6819e9c-9b68-4100-aed7-f188cbd69c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20832"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfa32527-3de8-4a27-ad54-815d1c92b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def pad_sequence(array: np.ndarray, max_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Принимает на вход массив массивов ``array`` и производит padding каждого вложенного массива до ``max_len``.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    array: numpy.ndarray\n",
    "        Входной массив массивов.\n",
    "    max_len: int\n",
    "        Длина, до которой нужно сделать padding вложенных массивов.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    output: numpy.ndarray\n",
    "        Выходной массив.\n",
    "    \"\"\"\n",
    "    if isinstance(max_len, float):\n",
    "        print(max_len)\n",
    "    output = np.zeros((len(features), max_len))\n",
    "    output[:, :array.shape[1]] = array\n",
    "    return output\n",
    "\n",
    "\n",
    "def truncate(x, num_last_credits: int = 0):\n",
    "    return pd.Series({\"sequences\": x.values.transpose()[:, -num_last_credits:]})\n",
    "\n",
    "\n",
    "def transform_credits_to_sequences(credits_frame: pd.DataFrame,\n",
    "                                   num_last_credits: int = 0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Принимает pandas.DataFrame с записями кредитных историй клиентов, сортирует кредиты по клиентам\n",
    "    (внутри клиента сортирует кредиты от старых к новым), берет ``num_last_credits`` кредитов,\n",
    "    возвращает новый pandas.DataFrame с двумя колонками: id и sequences.\n",
    "    Каждое значение в столбце sequences - это массив массивов.\n",
    "    Каждый вложенный массив - значение одного признака во всех кредитах клиента.\n",
    "    Всего признаков len(features), поэтому будет len(features) массивов.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    credits_frame: pandas.DataFrame\n",
    "        Датафрейм с записями кредитных историй клиентов.\n",
    "    num_last_credits: int, default=0\n",
    "         Количество кредитов клиента, которые будут включены в выходные данные. Если 0, то берутся все кредиты.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    output: pandas.DataFrame\n",
    "        Выходной датафрейм с двумя столбцами: \"id\", \"sequences\".\n",
    "    \"\"\"\n",
    "    return credits_frame \\\n",
    "        .sort_values([\"id\", \"rn\"]) \\\n",
    "        .groupby([\"id\"])[features] \\\n",
    "        .apply(lambda x: truncate(x, num_last_credits=num_last_credits)) \\\n",
    "        .reset_index()\n",
    "\n",
    "\n",
    "def create_padded_buckets(frame_of_sequences: pd.DataFrame, bucket_info: Dict[int, int],\n",
    "                          save_to_file_path: str = None, has_target: bool = True):\n",
    "    \"\"\"\n",
    "    Реализует Sequence Bucketing технику для обучения рекуррентных нейронных сетей.\n",
    "    Принимает на вход датафрейм ``frame_of_sequences`` с двумя столбцами: \"id\", \"sequences\"\n",
    "    (результат работы функции transform_credits_to_sequences),\n",
    "    словарь ``bucket_info``, где для последовательности каждой длины указано, до какой максимальной длины нужно делать\n",
    "    padding, группирует кредиты по бакетам (на основе длины), производит padding нулями и сохраняет результат\n",
    "    в pickle файл, если требуется.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    frame_of_sequences: pandas.DataFrame\n",
    "        Входной датафрейм с двумя столбцами \"id\", \"sequences\" (результат работы функции transform_credits_to_sequences).\n",
    "    bucket_info: Dict[int, int]\n",
    "        Cловарь, где для последовательности каждой длины указано, до какой максимальной длины нужно делать padding.\n",
    "    save_to_file_path: str, default=None\n",
    "        Опциональный путь до файла, куда нужно сохранить результат. Если None, то сохранение не требуется.\n",
    "    has_target: bool, deafult=True\n",
    "        Флаг, есть ли в frame_of_sequences целевая переменная или нет. Если есть, то она также будет записана в выходной словарь.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    dict_result: dict\n",
    "        Выходной словарь со ключами:  \"id\", \"padded_sequences\", \"target\".\n",
    "    \"\"\"\n",
    "    frame_of_sequences[\"sequence_length\"] = frame_of_sequences[\"sequences\"].apply(lambda x: len(x[1]))\n",
    "    frame_of_sequences[\"bucket_idx\"] = frame_of_sequences[\"sequence_length\"].map(bucket_info)\n",
    "    padded_seq = []\n",
    "    targets = []\n",
    "    ids = []\n",
    "\n",
    "    for size, bucket in tqdm.notebook.tqdm(frame_of_sequences.groupby(\"bucket_idx\"), desc=\"Extracting buckets\"):\n",
    "        padded_sequences = bucket[\"sequences\"].apply(lambda x: pad_sequence(x, size)).values\n",
    "        padded_seq.append(np.stack(padded_sequences, axis=0))\n",
    "\n",
    "        if has_target:\n",
    "            targets.append(bucket[\"flag\"].values)\n",
    "\n",
    "        ids.append(bucket[\"id\"].values)\n",
    "\n",
    "    frame_of_sequences.drop(columns=[\"bucket_idx\"], inplace=True)\n",
    "\n",
    "    dict_result = {\n",
    "        \"id\": np.array(ids, dtype=np.object_),\n",
    "        \"padded_sequences\": np.array(padded_seq, dtype=np.object_),\n",
    "        \"target\": np.array(targets, dtype=np.object_) if targets else []\n",
    "    }\n",
    "\n",
    "    if save_to_file_path:\n",
    "        with open(save_to_file_path, \"wb\") as f:\n",
    "            pickle.dump(dict_result, f)\n",
    "    return dict_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f18ccf65-db10-47df-aa17-15e911fee2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"pre_since_opened\", \"pre_since_confirmed\", \"pre_pterm\", \"pre_fterm\", \"pre_till_pclose\", \"pre_till_fclose\",\n",
    "            \"pre_loans_credit_limit\", \"pre_loans_next_pay_summ\", \"pre_loans_outstanding\", \"pre_loans_total_overdue\",\n",
    "            \"pre_loans_max_overdue_sum\", \"pre_loans_credit_cost_rate\",\n",
    "            \"pre_loans5\", \"pre_loans530\", \"pre_loans3060\", \"pre_loans6090\", \"pre_loans90\",\n",
    "            \"is_zero_loans5\", \"is_zero_loans530\", \"is_zero_loans3060\", \"is_zero_loans6090\", \"is_zero_loans90\",\n",
    "            \"pre_util\", \"pre_over2limit\", \"pre_maxover2limit\", \"is_zero_util\", \"is_zero_over2limit\", \"is_zero_maxover2limit\",\n",
    "            \"enc_paym_0\", \"enc_paym_1\", \"enc_paym_2\", \"enc_paym_3\", \"enc_paym_4\", \"enc_paym_5\", \"enc_paym_6\", \"enc_paym_7\", \"enc_paym_8\",\n",
    "            \"enc_paym_9\", \"enc_paym_10\", \"enc_paym_11\", \"enc_paym_12\", \"enc_paym_13\", \"enc_paym_14\", \"enc_paym_15\", \"enc_paym_16\",\n",
    "            \"enc_paym_17\", \"enc_paym_18\", \"enc_paym_19\", \"enc_paym_20\", \"enc_paym_21\", \"enc_paym_22\", \"enc_paym_23\", \"enc_paym_24\",\n",
    "            \"enc_loans_account_holder_type\", \"enc_loans_credit_status\", \"enc_loans_credit_type\", \"enc_loans_account_cur\",\n",
    "            \"pclose_flag\", \"fclose_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ed8d5ce-6e5c-4527-b4d0-9c0f20f86af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Count statistics on train data:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_1.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_2.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_3.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_4.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5fd377cb9f416c8f0c7e6231a06eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Count statistics on train data:  33%|███▎      | 1/3 [00:00<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_4.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_5.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_6.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_7.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c88babd6b04269a57196c1f333f437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Count statistics on train data:  67%|██████▋   | 2/3 [00:00<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_8.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_9.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_10.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_11.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d455420cc74bf4b86f76c48971dda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Count statistics on train data: 100%|██████████| 3/3 [00:01<00:00,  2.80it/s]\n",
      "Count statistics on test data:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/test_data/part_1.pqt\n",
      "/trinity/home/team08/workspace/main_project/test_data/part_2.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2df2badb297462190af639e6dce64ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Count statistics on test data: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.51 s, sys: 2.38 s, total: 6.89 s\n",
      "Wall time: 2.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "\n",
    "train_lens = []\n",
    "test_lens = []\n",
    "uniques = defaultdict(set)\n",
    "\n",
    "for step in tqdm.tqdm(range(0, 12, 4),\n",
    "                     desc=\"Count statistics on train data\"):\n",
    "        credits_frame = read_parquet_dataset_from_local(TRAIN_DATA_PATH, step, 4, verbose=True)\n",
    "        seq_lens = credits_frame.groupby(\"id\").agg(seq_len=(\"rn\", \"max\"))[\"seq_len\"].values\n",
    "        train_lens.extend(seq_lens)\n",
    "        credits_frame.drop(columns=[\"id\", \"rn\"], inplace=True)\n",
    "        for feat in credits_frame.columns.values:\n",
    "            uniques[feat] = uniques[feat].union(credits_frame[feat].unique())\n",
    "train_lens = np.hstack(train_lens)\n",
    "\n",
    "for step in tqdm.tqdm(range(0, 2, 2),\n",
    "                     desc=\"Count statistics on test data\"):\n",
    "        credits_frame = read_parquet_dataset_from_local(TEST_DATA_PATH, step, 2, verbose=True)\n",
    "        seq_lens = credits_frame.groupby(\"id\").agg(seq_len=(\"rn\", \"max\"))[\"seq_len\"].values\n",
    "        test_lens.extend(seq_lens)\n",
    "        credits_frame.drop(columns=[\"id\", \"rn\"], inplace=True)\n",
    "        for feat in credits_frame.columns.values:\n",
    "            uniques[feat] = uniques[feat].union(credits_frame[feat].unique())\n",
    "test_lens = np.hstack(test_lens)\n",
    "uniques = dict(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e4adf-2ddc-4cc0-872e-a99039933b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_len_counter = pd.Series(Counter(train_lens)).sort_index()\n",
    "test_len_counter = pd.Series(Counter(test_lens)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65e17f2d-5c97-47a8-af62-1c6de4897c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4dff9c7-2464-4187-81c8-40a814de39c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 46 artists>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNQAAAH5CAYAAABJQmPGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqlUlEQVR4nO3df5DWdb3H/Rew7kLKLqHCugMCHUv8BRombmlHk+NCezxx4jhpTpGHcnIWR9yThmc8SHbOwNFKMVGO0ylqRvJHM9pJDONgQiX+IjkpJxntwECDC6bBCneCwt5/dHPdbiL6gcUFeTxmrhn2+n6u7/W+1j7tzHO+13X16Ojo6AgAAAAA8I707O4BAAAAAOBAIqgBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKBAVXcP0J127NiRdevWpW/fvunRo0d3jwMAAABAN+no6Mgrr7yShoaG9Oy5+2vQDuqgtm7dugwePLi7xwAAAABgP7F27doMGjRot2sO6qDWt2/fJH/+RdXW1nbzNAAAAAB0l/b29gwePLjSi3bnoA5qO9/mWVtbK6gBAAAA8I4+FsyXEgAAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoIKgBAAAAQAFBDQAAAAAKCGoAAAAAUKCquwfg4DR06vy9Psfqmc1dMAkAAABAGVeoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoUNXdA7D/Gzp1/l6fY/XM5i6YBAAAAKD7uUINAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAU8KUEvGfs7Zcn+OIEAAAA4J1whRoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAJV3T0AXWvo1Pl7fY7VM5u7YBIAAACA9yZXqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAAChQFtRkzZuQjH/lI+vbtmwEDBmT8+PFZuXJlpzWvvvpqWlpacvjhh+ewww7LhAkTsn79+k5r1qxZk+bm5rzvfe/LgAEDcuWVV+b111/vtObhhx/Ohz/84dTU1OSYY47J3Llz3zTP7NmzM3To0PTu3TujR4/O448/XvJyAAAAAKBYUVBbvHhxWlpa8uijj2bhwoV57bXXcu6552bLli2VNVdccUV+8pOf5J577snixYuzbt26fPrTn64c3759e5qbm7Nt27Y88sgj+f73v5+5c+dm2rRplTWrVq1Kc3Nzzj777CxfvjxTpkzJF7/4xTz44IOVNXfddVdaW1tz7bXX5te//nVGjhyZpqambNiwYW9+HwAAAACwWz06Ojo69vTBL774YgYMGJDFixfn4x//eDZt2pQjjzwy8+bNyz/8wz8kSZ599tkcd9xxWbp0aU4//fT89Kc/zd/+7d9m3bp1GThwYJJkzpw5+epXv5oXX3wx1dXV+epXv5r58+fnmWeeqTzXBRdckI0bN2bBggVJktGjR+cjH/lIbrnlliTJjh07Mnjw4Fx22WWZOnXqO5q/vb09dXV12bRpU2pra/f017BfGTp1/l6fY/XM5oPynH95PgAAAODgUdKJ9uoz1DZt2pQk6d+/f5Jk2bJlee211zJmzJjKmuHDh+foo4/O0qVLkyRLly7NSSedVIlpSdLU1JT29vasWLGisuaN59i5Zuc5tm3blmXLlnVa07Nnz4wZM6ayZle2bt2a9vb2TjcAAAAAKLHHQW3Hjh2ZMmVKPvaxj+XEE09MkrS1taW6ujr9+vXrtHbgwIFpa2urrHljTNt5fOex3a1pb2/Pn/70p/zhD3/I9u3bd7lm5zl2ZcaMGamrq6vcBg8eXP7CAQAAADio7XFQa2lpyTPPPJM777yzK+fZp66++ups2rSpclu7dm13jwQAAADAAaZqTx40efLk3H///VmyZEkGDRpUub++vj7btm3Lxo0bO12ltn79+tTX11fW/OW3ce78FtA3rvnLbwZdv359amtr06dPn/Tq1Su9evXa5Zqd59iVmpqa1NTUlL9gAAAAAPj/FAW1jo6OXHbZZbn33nvz8MMPZ9iwYZ2Ojxo1KoccckgWLVqUCRMmJElWrlyZNWvWpLGxMUnS2NiYf/u3f8uGDRsyYMCAJMnChQtTW1ub448/vrLmgQce6HTuhQsXVs5RXV2dUaNGZdGiRRk/fnySP78FddGiRZk8eXLhrwDemi86AAAAAP5SUVBraWnJvHnz8uMf/zh9+/atfF5ZXV1d+vTpk7q6ukyaNCmtra3p379/amtrc9lll6WxsTGnn356kuTcc8/N8ccfn8997nO5/vrr09bWlmuuuSYtLS2Vq8e+/OUv55ZbbslVV12Vf/zHf8xDDz2Uu+++O/Pn//9xo7W1NRMnTsypp56a0047LTfddFO2bNmSiy++uKt+NwAAAADwJkVB7bbbbkuSnHXWWZ3u/973vpcvfOELSZIbb7wxPXv2zIQJE7J169Y0NTXl1ltvrazt1atX7r///lx66aVpbGzMoYcemokTJ+a6666rrBk2bFjmz5+fK664IrNmzcqgQYPyne98J01NTZU1n/nMZ/Liiy9m2rRpaWtry8knn5wFCxa86YsKAAAAAKArFb/l8+307t07s2fPzuzZs99yzZAhQ970ls6/dNZZZ+Wpp57a7ZrJkyd7iycAAAAA76o9/pZPAAAAADgYCWoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFqrp7ADiYDJ06f6/PsXpmcxdMAgAAAOwpV6gBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoIKgBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAAChQ1d0DAHtn6NT5e32O1TObu2ASAAAAODi4Qg0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoIKgBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoIKgBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKBAVXcPAOx/hk6dv9fnWD2zuQsmAQAAgP2PK9QAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAECB4qC2ZMmSnHfeeWloaEiPHj1y3333dTr+hS98IT169Oh0Gzt2bKc1L7/8ci666KLU1tamX79+mTRpUjZv3txpzW9+85uceeaZ6d27dwYPHpzrr7/+TbPcc889GT58eHr37p2TTjopDzzwQOnLAQAAAIAixUFty5YtGTlyZGbPnv2Wa8aOHZsXXnihcvvhD3/Y6fhFF12UFStWZOHChbn//vuzZMmSXHLJJZXj7e3tOffcczNkyJAsW7YsN9xwQ6ZPn57bb7+9suaRRx7JhRdemEmTJuWpp57K+PHjM378+DzzzDOlLwkAAAAA3rGq0geMGzcu48aN2+2ampqa1NfX7/LYb3/72yxYsCBPPPFETj311CTJt7/97Xzyk5/MN77xjTQ0NOSOO+7Itm3b8t3vfjfV1dU54YQTsnz58nzrW9+qhLdZs2Zl7NixufLKK5MkX//617Nw4cLccsstmTNnzi6fe+vWrdm6dWvl5/b29tKXDwAAAMBBbp98htrDDz+cAQMG5Nhjj82ll16al156qXJs6dKl6devXyWmJcmYMWPSs2fPPPbYY5U1H//4x1NdXV1Z09TUlJUrV+aPf/xjZc2YMWM6PW9TU1OWLl36lnPNmDEjdXV1ldvgwYO75PUCAAAAcPDo8qA2duzY/OAHP8iiRYvy7//+71m8eHHGjRuX7du3J0na2toyYMCATo+pqqpK//7909bWVlkzcODATmt2/vx2a3Ye35Wrr746mzZtqtzWrl27dy8WAAAAgINO8Vs+384FF1xQ+fdJJ52UESNG5K/+6q/y8MMP55xzzunqpytSU1OTmpqabp0BAAAAgAPbPnnL5xt94AMfyBFHHJHnn38+SVJfX58NGzZ0WvP666/n5ZdfrnzuWn19fdavX99pzc6f327NW312GwAAAAB0hX0e1H7/+9/npZdeylFHHZUkaWxszMaNG7Ns2bLKmoceeig7duzI6NGjK2uWLFmS1157rbJm4cKFOfbYY/P+97+/smbRokWdnmvhwoVpbGzc1y8JAAAAgINYcVDbvHlzli9fnuXLlydJVq1aleXLl2fNmjXZvHlzrrzyyjz66KNZvXp1Fi1alE996lM55phj0tTUlCQ57rjjMnbs2HzpS1/K448/nl/96leZPHlyLrjggjQ0NCRJPvvZz6a6ujqTJk3KihUrctddd2XWrFlpbW2tzHH55ZdnwYIF+eY3v5lnn30206dPz5NPPpnJkyd3wa8FAAAAAHatOKg9+eSTOeWUU3LKKackSVpbW3PKKadk2rRp6dWrV37zm9/k7/7u7/KhD30okyZNyqhRo/KLX/yi02eX3XHHHRk+fHjOOeecfPKTn8wZZ5yR22+/vXK8rq4uP/vZz7Jq1aqMGjUq//RP/5Rp06blkksuqaz56Ec/mnnz5uX222/PyJEj86Mf/Sj33XdfTjzxxL35fQAAAADAbhV/KcFZZ52Vjo6Otzz+4IMPvu05+vfvn3nz5u12zYgRI/KLX/xit2vOP//8nH/++W/7fAAAAADQVfb5Z6gBAAAAwHuJoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAWqunsA4OAwdOr8vT7H6pnNXTAJAAAA7B1XqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoIKgBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAECBqu4eAGBPDZ06f68ev3pmcxdNAgAAwMHEFWoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoIKgBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABaq6ewCA/cXQqfP3+hyrZzZ3wSQAAADsz1yhBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAsVBbcmSJTnvvPPS0NCQHj165L777ut0vKOjI9OmTctRRx2VPn36ZMyYMXnuuec6rXn55Zdz0UUXpba2Nv369cukSZOyefPmTmt+85vf5Mwzz0zv3r0zePDgXH/99W+a5Z577snw4cPTu3fvnHTSSXnggQdKXw4AAAAAFCkOalu2bMnIkSMze/bsXR6//vrrc/PNN2fOnDl57LHHcuihh6apqSmvvvpqZc1FF12UFStWZOHChbn//vuzZMmSXHLJJZXj7e3tOffcczNkyJAsW7YsN9xwQ6ZPn57bb7+9suaRRx7JhRdemEmTJuWpp57K+PHjM378+DzzzDOlLwkAAAAA3rGq0geMGzcu48aN2+Wxjo6O3HTTTbnmmmvyqU99Kknygx/8IAMHDsx9992XCy64IL/97W+zYMGCPPHEEzn11FOTJN/+9rfzyU9+Mt/4xjfS0NCQO+64I9u2bct3v/vdVFdX54QTTsjy5cvzrW99qxLeZs2albFjx+bKK69Mknz961/PwoULc8stt2TOnDl79MsAAAAAgLfTpZ+htmrVqrS1tWXMmDGV++rq6jJ69OgsXbo0SbJ06dL069evEtOSZMyYMenZs2cee+yxypqPf/zjqa6urqxpamrKypUr88c//rGy5o3Ps3PNzufZla1bt6a9vb3TDQAAAABKdGlQa2trS5IMHDiw0/0DBw6sHGtra8uAAQM6Ha+qqkr//v07rdnVOd74HG+1ZufxXZkxY0bq6uoqt8GDB5e+RAAAAAAOcgfVt3xeffXV2bRpU+W2du3a7h4JAAAAgANMlwa1+vr6JMn69es73b9+/frKsfr6+mzYsKHT8ddffz0vv/xypzW7Oscbn+Ot1uw8vis1NTWpra3tdAMAAACAEl0a1IYNG5b6+vosWrSocl97e3see+yxNDY2JkkaGxuzcePGLFu2rLLmoYceyo4dOzJ69OjKmiVLluS1116rrFm4cGGOPfbYvP/976+seePz7Fyz83kAAAAAYF8oDmqbN2/O8uXLs3z58iR//iKC5cuXZ82aNenRo0emTJmSf/3Xf81//dd/5emnn87nP//5NDQ0ZPz48UmS4447LmPHjs2XvvSlPP744/nVr36VyZMn54ILLkhDQ0OS5LOf/Wyqq6szadKkrFixInfddVdmzZqV1tbWyhyXX355FixYkG9+85t59tlnM3369Dz55JOZPHny3v9WAAAAAOAtVJU+4Mknn8zZZ59d+Xln5Jo4cWLmzp2bq666Klu2bMkll1ySjRs35owzzsiCBQvSu3fvymPuuOOOTJ48Oeecc0569uyZCRMm5Oabb64cr6ury89+9rO0tLRk1KhROeKIIzJt2rRccskllTUf/ehHM2/evFxzzTX553/+53zwgx/MfffdlxNPPHGPfhEAAAAA8E4UB7WzzjorHR0db3m8R48eue6663Lddde95Zr+/ftn3rx5u32eESNG5Be/+MVu15x//vk5//zzdz8wAAAAAHShg+pbPgEAAABgbwlqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoIKgBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAAChQ1d0DALyXDZ06f6/PsXpmcxdMAgAAQFdxhRoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoIKgBAAAAQAFBDQAAAAAKCGoAAAAAUKCquwcAoMzQqfP3+hyrZzZ3wSQAAAAHJ1eoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoIKgBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAAClR19wAAdL+hU+fv9TlWz2zugkkAAAD2f65QAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoEBVdw8AwHvT0Knz9+rxq2c2d9EkAAAAXcsVagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABTo8qA2ffr09OjRo9Nt+PDhleOvvvpqWlpacvjhh+ewww7LhAkTsn79+k7nWLNmTZqbm/O+970vAwYMyJVXXpnXX3+905qHH344H/7wh1NTU5Njjjkmc+fO7eqXAgAAAABvsk+uUDvhhBPywgsvVG6//OUvK8euuOKK/OQnP8k999yTxYsXZ926dfn0pz9dOb59+/Y0Nzdn27ZteeSRR/L9738/c+fOzbRp0yprVq1alebm5px99tlZvnx5pkyZki9+8Yt58MEH98XLAQAAAICKqn1y0qqq1NfXv+n+TZs25T//8z8zb968fOITn0iSfO9738txxx2XRx99NKeffnp+9rOf5X//93/z3//93xk4cGBOPvnkfP3rX89Xv/rVTJ8+PdXV1ZkzZ06GDRuWb37zm0mS4447Lr/85S9z4403pqmpaV+8JAAAAABIso+C2nPPPZeGhob07t07jY2NmTFjRo4++ugsW7Ysr732WsaMGVNZO3z48Bx99NFZunRpTj/99CxdujQnnXRSBg4cWFnT1NSUSy+9NCtWrMgpp5ySpUuXdjrHzjVTpkzZ7Vxbt27N1q1bKz+3t7d3zQsGYJ8bOnX+Xp9j9czmLpgEAAA42HX5Wz5Hjx6duXPnZsGCBbntttuyatWqnHnmmXnllVfS1taW6urq9OvXr9NjBg4cmLa2tiRJW1tbp5i28/jOY7tb097enj/96U9vOduMGTNSV1dXuQ0ePHhvXy4AAAAAB5kuv0Jt3LhxlX+PGDEio0ePzpAhQ3L33XenT58+Xf10Ra6++uq0trZWfm5vbxfVAAAAACiyT76U4I369euXD33oQ3n++edTX1+fbdu2ZePGjZ3WrF+/vvKZa/X19W/61s+dP7/dmtra2t1Gu5qamtTW1na6AQAAAECJfR7UNm/enN/97nc56qijMmrUqBxyyCFZtGhR5fjKlSuzZs2aNDY2JkkaGxvz9NNPZ8OGDZU1CxcuTG1tbY4//vjKmjeeY+eanecAAAAAgH2ly4PaV77ylSxevDirV6/OI488kr//+79Pr169cuGFF6auri6TJk1Ka2trfv7zn2fZsmW5+OKL09jYmNNPPz1Jcu655+b444/P5z73ufzP//xPHnzwwVxzzTVpaWlJTU1NkuTLX/5y/u///i9XXXVVnn322dx66625++67c8UVV3T1ywEAAACATrr8M9R+//vf58ILL8xLL72UI488MmeccUYeffTRHHnkkUmSG2+8MT179syECROydevWNDU15dZbb608vlevXrn//vtz6aWXprGxMYceemgmTpyY6667rrJm2LBhmT9/fq644orMmjUrgwYNyne+8500NTV19csBAAAAgE66PKjdeeeduz3eu3fvzJ49O7Nnz37LNUOGDMkDDzyw2/OcddZZeeqpp/ZoRgAAAADYU/v8M9QAAAAA4L1EUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAUENQAAAAAoUNXdAwBAdxk6df5en2P1zOYumAQAADiQuEINAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFCgqrsHAID3kqFT5+/1OVbPbO6CSQAAgH3FFWoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAAUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFqrp7AABg94ZOnb/X51g9s7kLJgEAABJXqAEAAABAEUENAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAECBqu4eAAB49w2dOn+vHr96ZnMXTQIAAAceV6gBAAAAQAFBDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABQQ1AAAAACggqAEAAABAgaruHgAAeG8YOnX+Xj1+9czmLpoEAAD2LVeoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAoIaAAAAABQQ1AAAAACggKAGAAAAAAWqunsAAIBdGTp1/l6fY/XM5i6YBAAAOnOFGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAAClR19wAAAO+WoVPn7/U5Vs9s7oJJAAA4kLlCDQAAAAAKCGoAAAAAUEBQAwAAAIACghoAAAAAFBDUAAAAAKCAoAYAAAAABaq6ewAAgAPZ0Knz9/ocq2c2d8EkAAC8W1yhBgAAAAAFBDUAAAAAKCCoAQAAAEABn6EGALCf2dvPZfOZbAAA+5Yr1AAAAACggKAGAAAAAAUENQAAAAAo4DPUAAAOAj6XDQCg67hCDQAAAAAKCGoAAAAAUEBQAwAAAIACPkMNAIBie/uZbInPZQMADlyCGgAA+wWRDgA4UHjLJwAAAAAUcIUaAADvWa56AwD2BVeoAQAAAECBA/4KtdmzZ+eGG25IW1tbRo4cmW9/+9s57bTTunssAADeo/b2qjdXvAHAge+ADmp33XVXWltbM2fOnIwePTo33XRTmpqasnLlygwYMKC7xwMAgHekqyOdt7oCwL51QAe1b33rW/nSl76Uiy++OEkyZ86czJ8/P9/97nczderUN63funVrtm7dWvl506ZNSZL29vZ3Z+B3wY6t/89en+Mvfx8Hyzl39b+Drj7n/vi6D+ZzHggzHsznPBBmPJjP6f8z9+9z+u+zf5/zQP3vc+K1D+71OZ/5WtM+PycA7Kmdf/s6Ojredm2Pjneyaj+0bdu2vO9978uPfvSjjB8/vnL/xIkTs3Hjxvz4xz9+02OmT5+er33ta+/ilAAAAAAcSNauXZtBgwbtds0Be4XaH/7wh2zfvj0DBw7sdP/AgQPz7LPP7vIxV199dVpbWys/79ixIy+//HIOP/zw9OjRY5/Ouyfa29szePDgrF27NrW1td09DhxQ7B/Yc/YP7Dn7B/ac/QN7zv7pGh0dHXnllVfS0NDwtmsP2KC2J2pqalJTU9Ppvn79+nXPMAVqa2ttCNhD9g/sOfsH9pz9A3vO/oE9Z//svbq6une0ruc+nmOfOeKII9KrV6+sX7++0/3r169PfX19N00FAAAAwHvdARvUqqurM2rUqCxatKhy344dO7Jo0aI0NjZ242QAAAAAvJcd0G/5bG1tzcSJE3PqqafmtNNOy0033ZQtW7ZUvvXzQFdTU5Nrr732TW9TBd6e/QN7zv6BPWf/wJ6zf2DP2T/vvgP2Wz53uuWWW3LDDTekra0tJ598cm6++eaMHj26u8cCAAAA4D3qgA9qAAAAAPBuOmA/Qw0AAAAAuoOgBgAAAAAFBDUAAAAAKCCoAQAAAEABQW0/NXv27AwdOjS9e/fO6NGj8/jjj3f3SLBfWrJkSc4777w0NDSkR48eue+++zod7+joyLRp03LUUUelT58+GTNmTJ577rnuGRb2IzNmzMhHPvKR9O3bNwMGDMj48eOzcuXKTmteffXVtLS05PDDD89hhx2WCRMmZP369d00Mew/brvttowYMSK1tbWpra1NY2NjfvrTn1aO2zvwzs2cOTM9evTIlClTKvfZQ7Br06dPT48ePTrdhg8fXjlu77y7BLX90F133ZXW1tZce+21+fWvf52RI0emqakpGzZs6O7RYL+zZcuWjBw5MrNnz97l8euvvz4333xz5syZk8ceeyyHHnpompqa8uqrr77Lk8L+ZfHixWlpacmjjz6ahQsX5rXXXsu5556bLVu2VNZcccUV+clPfpJ77rknixcvzrp16/LpT3+6G6eG/cOgQYMyc+bMLFu2LE8++WQ+8YlP5FOf+lRWrFiRxN6Bd+qJJ57If/zHf2TEiBGd7reH4K2dcMIJeeGFFyq3X/7yl5Vj9s67rIP9zmmnndbR0tJS+Xn79u0dDQ0NHTNmzOjGqWD/l6Tj3nvvrfy8Y8eOjvr6+o4bbrihct/GjRs7ampqOn74wx92w4Sw/9qwYUNHko7Fixd3dHT8ea8ccsghHffcc09lzW9/+9uOJB1Lly7trjFhv/X+97+/4zvf+Y69A+/QK6+80vHBD36wY+HChR1//dd/3XH55Zd3dHT4+wO7c+2113aMHDlyl8fsnXefK9T2M9u2bcuyZcsyZsyYyn09e/bMmDFjsnTp0m6cDA48q1atSltbW6f9VFdXl9GjR9tP8Bc2bdqUJOnfv3+SZNmyZXnttdc67Z/hw4fn6KOPtn/gDbZv354777wzW7ZsSWNjo70D71BLS0uam5s77ZXE3x94O88991waGhrygQ98IBdddFHWrFmTxN7pDlXdPQCd/eEPf8j27dszcODATvcPHDgwzz77bDdNBQemtra2JNnlftp5DEh27NiRKVOm5GMf+1hOPPHEJH/eP9XV1enXr1+ntfYP/NnTTz+dxsbGvPrqqznssMNy77335vjjj8/y5cvtHXgbd955Z37961/niSeeeNMxf3/grY0ePTpz587NsccemxdeeCFf+9rXcuaZZ+aZZ56xd7qBoAYAB7mWlpY888wznT6DA9i9Y489NsuXL8+mTZvyox/9KBMnTszixYu7eyzY761duzaXX355Fi5cmN69e3f3OHBAGTduXOXfI0aMyOjRozNkyJDcfffd6dOnTzdOdnDyls/9zBFHHJFevXq96Zs41q9fn/r6+m6aCg5MO/eM/QRvbfLkybn//vvz85//PIMGDarcX19fn23btmXjxo2d1ts/8GfV1dU55phjMmrUqMyYMSMjR47MrFmz7B14G8uWLcuGDRvy4Q9/OFVVVamqqsrixYtz8803p6qqKgMHDrSH4B3q169fPvShD+X555/396cbCGr7merq6owaNSqLFi2q3Ldjx44sWrQojY2N3TgZHHiGDRuW+vr6Tvupvb09jz32mP3EQa+joyOTJ0/Ovffem4ceeijDhg3rdHzUqFE55JBDOu2flStXZs2aNfYP7MKOHTuydetWewfexjnnnJOnn346y5cvr9xOPfXUXHTRRZV/20PwzmzevDm/+93vctRRR/n70w285XM/1NramokTJ+bUU0/NaaedlptuuilbtmzJxRdf3N2jwX5n8+bNef755ys/r1q1KsuXL0///v1z9NFHZ8qUKfnXf/3XfPCDH8ywYcPyL//yL2loaMj48eO7b2jYD7S0tGTevHn58Y9/nL59+1Y+W6Ouri59+vRJXV1dJk2alNbW1vTv3z+1tbW57LLL0tjYmNNPP72bp4fudfXVV2fcuHE5+uij88orr2TevHl5+OGH8+CDD9o78Db69u1b+bzOnQ499NAcfvjhlfvtIdi1r3zlKznvvPMyZMiQrFu3Ltdee2169eqVCy+80N+fbiCo7Yc+85nP5MUXX8y0adPS1taWk08+OQsWLHjTB6sDyZNPPpmzzz678nNra2uSZOLEiZk7d26uuuqqbNmyJZdcckk2btyYM844IwsWLPCZHRz0brvttiTJWWed1en+733ve/nCF76QJLnxxhvTs2fPTJgwIVu3bk1TU1NuvfXWd3lS2P9s2LAhn//85/PCCy+krq4uI0aMyIMPPpi/+Zu/SWLvwN6yh2DXfv/73+fCCy/MSy+9lCOPPDJnnHFGHn300Rx55JFJ7J13W4+Ojo6O7h4CAAAAAA4UPkMNAAAAAAoIagAAAABQQFADAAAAgAKCGgAAAAAUENQAAAAAoICgBgAAAAAFBDUAAAAAKCCoAQAAAEABQQ0AAAAACghqAAAAAFBAUAMAAACAAv8vIpWx0kTit0EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6), nrows=1)\n",
    "ax.bar(train_len_counter.index.values, train_len_counter.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb884a8d-8309-437b-b79b-fe9ac30fa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_buckets_from_credits(path_to_dataset, bucket_info, save_to_path, frame_with_ids = None,\n",
    "                                num_parts_to_preprocess_at_once: int = 1,\n",
    "                                num_parts_total=50, has_target=False):\n",
    "    print('test_1')\n",
    "    block = 0\n",
    "    for step in tqdm.tqdm(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
    "                     desc=\"Preparing credit data\"):\n",
    "        print('test_2')\n",
    "        credits_frame = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_to_preprocess_at_once, verbose=True)\n",
    "        print('test_3')\n",
    "        credits_frame.loc[:, features] += 1\n",
    "        print('test_4')\n",
    "        seq = transform_credits_to_sequences(credits_frame)\n",
    "        print('test_5')\n",
    "        print(\"Transforming credits to sequences is done.\")\n",
    "\n",
    "        if frame_with_ids is not None:\n",
    "            print('test_6')\n",
    "            seq = seq.merge(frame_with_ids, on=\"id\")\n",
    "            print('test_7')\n",
    "        print('test_8')\n",
    "        block_as_str = str(block)\n",
    "        print('test_9')\n",
    "        if len(block_as_str) == 1:\n",
    "            print('test_10')\n",
    "            block_as_str = \"00\" + block_as_str\n",
    "            print('test_11')\n",
    "        else:\n",
    "            print('test_12')\n",
    "            block_as_str = \"0\" + block_as_str\n",
    "            print('test_13')\n",
    "        print('test_14')\n",
    "\n",
    "        processed_fragment =  create_padded_buckets(seq, bucket_info=bucket_info, has_target=has_target, \n",
    "                                                    save_to_file_path=os.path.join(save_to_path, \n",
    "                                                                                   f\"processed_chunk_{block_as_str}.pkl\"))\n",
    "        block += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5a82de0-d468-4a5f-9581-2e97e790a6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18748, 2), (2084, 2))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val = train_test_split(train_target, random_state=42, test_size=0.1)\n",
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0f94b4f-8e8f-417a-8d2c-affbb6a0f2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 11: 11,\n",
       " 12: 12,\n",
       " 13: 13,\n",
       " 14: 14,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 45,\n",
       " 42: 45,\n",
       " 43: 45,\n",
       " 44: 45,\n",
       " 45: 45,\n",
       " 46: 50,\n",
       " 47: 50,\n",
       " 48: 50,\n",
       " 49: 50,\n",
       " 50: 50,\n",
       " 51: 58,\n",
       " 52: 58,\n",
       " 53: 58,\n",
       " 54: 58,\n",
       " 55: 58,\n",
       " 56: 58,\n",
       " 57: 58,\n",
       " 58: 58}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_ = list(range(1, 59))\n",
    "lens_ = list(range(1, 41)) + [45] * 5 + [50] * 5 + [58] * 8\n",
    "bucket_info = dict(zip(keys_, lens_))\n",
    "bucket_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab54bdcd-8759-440f-b3e6-eb2f3d7131f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2\n",
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_1.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_2.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_3.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_4.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a66ab206ce429f9c45f002dedbfef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_3\n",
      "test_4\n",
      "test_5\n",
      "Transforming credits to sequences is done.\n",
      "test_6\n",
      "test_7\n",
      "test_8\n",
      "test_9\n",
      "test_10\n",
      "test_11\n",
      "test_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c632bc999f4476865685c0c6c5d835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data:  33%|███▎      | 1/3 [00:12<00:24, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2\n",
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_4.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_5.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_6.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_7.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07af20e094be45dfac73af9142f9e770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_3\n",
      "test_4\n",
      "test_5\n",
      "Transforming credits to sequences is done.\n",
      "test_6\n",
      "test_7\n",
      "test_8\n",
      "test_9\n",
      "test_10\n",
      "test_11\n",
      "test_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69b986651a948488661d0202ce37edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data:  67%|██████▋   | 2/3 [00:23<00:11, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2\n",
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_8.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_9.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_10.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_11.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180a098a37c84876a5714a27488171ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_3\n",
      "test_4\n",
      "test_5\n",
      "Transforming credits to sequences is done.\n",
      "test_6\n",
      "test_7\n",
      "test_8\n",
      "test_9\n",
      "test_10\n",
      "test_11\n",
      "test_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83b77bf16b44080ade13c879d1270e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data: 100%|██████████| 3/3 [00:36<00:00, 12.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/trinity/home/team08/workspace/main_project/train_data/part_1.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_10.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_11.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_12.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_2.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_3.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_4.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_5.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_6.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_7.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_8.pqt',\n",
       " '/trinity/home/team08/workspace/main_project/train_data/part_9.pqt']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_BUCKETS_PATH = \"/trinity/home/team08/workspace/main_project/train_buckets_rnn\"\n",
    "VAL_BUCKETS_PATH = \"/trinity/home/team08/workspace/main_project/val_buckets_rnn\"\n",
    "TEST_BUCKETS_PATH = \"/trinity/home/team08/workspace/main_project/test_buckets_rnn\"\n",
    "\n",
    "\n",
    "for buckets_path in [TRAIN_BUCKETS_PATH, VAL_BUCKETS_PATH, TEST_BUCKETS_PATH]:\n",
    "    !rm -rf $buckets_path\n",
    "    !mkdir $buckets_path\n",
    "\n",
    "\n",
    "create_buckets_from_credits(TRAIN_DATA_PATH,\n",
    "                            bucket_info=bucket_info,\n",
    "                            save_to_path=TRAIN_BUCKETS_PATH,\n",
    "                            frame_with_ids=train,\n",
    "                            num_parts_to_preprocess_at_once=4,\n",
    "                            num_parts_total=12, has_target=True)\n",
    "\n",
    "dataset_train = sorted([os.path.join(TRAIN_DATA_PATH, x) for x in os.listdir(TRAIN_DATA_PATH)])\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ad23633-b724-4c4f-8a34-d07b9a87cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2\n",
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_1.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_2.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_3.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_4.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a027e12dc8004dbdabb9c709350f90d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_3\n",
      "test_4\n",
      "test_5\n",
      "Transforming credits to sequences is done.\n",
      "test_6\n",
      "test_7\n",
      "test_8\n",
      "test_9\n",
      "test_10\n",
      "test_11\n",
      "test_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d57d29a1404bff86926701c79d9e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data:  33%|███▎      | 1/3 [00:11<00:23, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2\n",
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_4.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_5.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_6.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_7.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785d5e06e53d4fa5a6ea8db709780be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_3\n",
      "test_4\n",
      "test_5\n",
      "Transforming credits to sequences is done.\n",
      "test_6\n",
      "test_7\n",
      "test_8\n",
      "test_9\n",
      "test_10\n",
      "test_11\n",
      "test_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cafd848300c40abb73b052cbf15e2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data:  67%|██████▋   | 2/3 [00:25<00:12, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2\n",
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_8.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_9.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_10.pqt\n",
      "/trinity/home/team08/workspace/main_project/train_data/part_11.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa53eafe804d4810afbd52ef65d8ef1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_3\n",
      "test_4\n",
      "test_5\n",
      "Transforming credits to sequences is done.\n",
      "test_6\n",
      "test_7\n",
      "test_8\n",
      "test_9\n",
      "test_10\n",
      "test_11\n",
      "test_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96ebec4a72f49eca64c9725051ee0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data: 100%|██████████| 3/3 [00:36<00:00, 12.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/trinity/home/team08/workspace/main_project/val_buckets_rnn/processed_chunk_000.pkl',\n",
       " '/trinity/home/team08/workspace/main_project/val_buckets_rnn/processed_chunk_001.pkl',\n",
       " '/trinity/home/team08/workspace/main_project/val_buckets_rnn/processed_chunk_002.pkl']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_buckets_from_credits(TRAIN_DATA_PATH,\n",
    "                            bucket_info=bucket_info,\n",
    "                            save_to_path=VAL_BUCKETS_PATH,\n",
    "                            frame_with_ids=val,\n",
    "                            num_parts_to_preprocess_at_once=4, \n",
    "                            num_parts_total=12, has_target=True)\n",
    "\n",
    "dataset_val = sorted([os.path.join(VAL_BUCKETS_PATH, x) for x in os.listdir(VAL_BUCKETS_PATH)])\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "147bca81-fcaa-4a39-a3c4-0da12b55de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2\n",
      "Reading chunks:\n",
      "/trinity/home/team08/workspace/main_project/test_data/part_1.pqt\n",
      "/trinity/home/team08/workspace/main_project/test_data/part_2.pqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/2522210297.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98de4a2492b14ae4928a6cb6a000541a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_3\n",
      "test_4\n",
      "test_5\n",
      "Transforming credits to sequences is done.\n",
      "test_8\n",
      "test_9\n",
      "test_10\n",
      "test_11\n",
      "test_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c75834a3544902a26fe02832486620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing credit data: 100%|██████████| 1/1 [00:40<00:00, 40.50s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/trinity/home/team08/workspace/main_project/test_buckets_rnn/processed_chunk_000.pkl']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_buckets_from_credits(TEST_DATA_PATH,\n",
    "                            bucket_info=bucket_info,\n",
    "                            save_to_path= TEST_BUCKETS_PATH, num_parts_to_preprocess_at_once=2, \n",
    "                            num_parts_total=2)\n",
    "\n",
    "dataset_test = sorted([os.path.join(TEST_BUCKETS_PATH, x) for x in os.listdir(TEST_BUCKETS_PATH)])\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0597589-dde6-4194-bb05-465250c1a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6928283-c9ff-452c-9422-a91198f82925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20cc4373-f025-4d80-9741-3782fdc2db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###from training_aux import EarlyStopping\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Останавливает обучение модели, если валидационная метрика не улучшается в течение заданного числа эпох.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    patience: int, default=7\n",
    "        Допустимое число эпох без улучшения валидационной метрики.\n",
    "        Валидационная метрика должна улучшаться как минимум каждые ``patience`` эпох, иначе обучение останавливается.\n",
    "    mode: str, default=\"min\"\n",
    "        Режим работы. Допустимые значения: \"min\", \"max\" - минимизация или максимизация целевой метрики соответственно.\n",
    "    verbose: bool, default=False\n",
    "        Печатать ли сообщение при каждом улучшении валидационной метрики.\n",
    "    delta: int, default=0\n",
    "        Минимальное изменение контролируемой метрики, которое можно считать улучшением.\n",
    "    save_path: str, default=\"checkpoint.hdf5\"\n",
    "        Путь до файла, в который необходимо сохранять лучшую модель.\n",
    "    metric_name: str, default=None\n",
    "        Имя метрики.\n",
    "    save_format: str, default=\"torch\"\n",
    "        Формат модели. Допустимые значения: \"torch\", \"tf\" - для моделей на фреймворках pytorch и tensorflow.keras соответственно.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, mode='min', verbose=False, delta=0, save_path='checkpoint.hdf5', metric_name=None, save_format='torch'):\n",
    "        if mode not in [\"min\", \"max\"]:\n",
    "            raise ValueError(f\"Unrecognized mode: {mode}! Please choose one of the following modes: \\\"min\\\", \\\"max\\\"\")\n",
    "\n",
    "        if save_format not in [\"torch\", \"tf\"]:\n",
    "            raise ValueError(f\"Unrecognized format: {save_format}! Please choose one of the following formats: \\\"torch\\\", \\\"tf\\\"\")\n",
    "\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_prev_score = np.Inf if mode == \"min\" else -np.Inf\n",
    "        self.delta = delta\n",
    "        self.save_path = save_path\n",
    "        self.metric_name = \"metric\" if not metric_name else metric_name\n",
    "        self.save_format = save_format\n",
    "\n",
    "    def __call__(self, metric_value, model):\n",
    "\n",
    "        score = -metric_value if self.mode == \"min\" else metric_value\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(metric_value, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                f\"No imporvement in validation {self.metric_name}. Current: {score:.6f}. Current best: {self.best_score:.6f}\")\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(metric_value, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, metric_value: float, model: torch.nn.Module or tensorflow.keras.Model):\n",
    "        \"\"\"\n",
    "        Cохраняет модель, если валидационная метрика улучшилась.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        metric_value: float\n",
    "            Значение валидационной метрики.\n",
    "        model: torch.nn.Module or tensorflow.keras.Model\n",
    "            Обучаемая модель.\n",
    "\n",
    "        Возвращаемое значение:\n",
    "        ----------------------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"Validation {self.metric_name} improved ({self.best_prev_score:.6f} --> {metric_value:.6f}).  Saving model...\")\n",
    "        if self.save_format == \"tf\":\n",
    "            model.save_weights(self.save_path)\n",
    "        else:\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "\n",
    "        self.best_prev_score = metric_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8464597e-bca1-41bf-a0b2-847d59a9b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###from data_generators import batches_generator\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "def batches_generator(list_of_paths: List[str], batch_size: int = 32, shuffle: bool = False,\n",
    "                      is_infinite: bool = False, verbose: bool = False, device: torch.device = None,\n",
    "                      output_format: str = \"torch\", is_train: bool = True):\n",
    "    \"\"\"\n",
    "    Создает батчи на вход рекуррентных нейронных сетей, реализованных на фреймворках tensorflow и pytorch.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    list_of_paths: List[str]\n",
    "        Список путей до файлов с предобработанными последовательностями.\n",
    "    batch_size: int, default=32\n",
    "        Размер батча.\n",
    "    shuffle: bool, default=False\n",
    "        Перемешивать ли данные перед генерацией батчей.\n",
    "    is_infinite: bool, default=False\n",
    "        Должен ли генератор быть бесконечным.\n",
    "    verbose: bool, default=False\n",
    "        Печатать ли имя текущего обрабатываемого файла.\n",
    "    device: torch.device, default=None\n",
    "        Девайс, на который переместить данные при ``output_format``=\"torch\". Игнорируется, если ``output_format``=\"tf\".\n",
    "    output_format: str, default=\"torch\"\n",
    "        Формат возвращаемых данных. Допустимые значения: \"torch\", \"tf\".\n",
    "        Если \"torch\", то возвращает словарь, с ключами \"id_\", \"features\" и \"label\", если is_train=True,\n",
    "        и содержащий идентификаторы заявок, признаки и тагрет соответственно.\n",
    "        Признаки и таргет помещаются на девайс, указанный в ``device``.\n",
    "        Если \"tf\", то возращает кортеж (признаки, таргет), если ``is_train``=True, и кортеж (признаки, идентификаторы заявок) иначе.\n",
    "    is_train: bool, default=True\n",
    "        Используется ли генератор для обучения модели или для инференса.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    result: dict or tuple\n",
    "        Выходной словарь или кортеж в зависимости от параметра ``output_format``.\n",
    "    \"\"\"\n",
    "    if output_format not in [\"torch\", \"tf\"]:\n",
    "        raise ValueError(\"Unknown format. Please choose one of the following formats: \\\"torch\\\", \\\"tf\\\"\")\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(list_of_paths)\n",
    "\n",
    "        for path in list_of_paths:\n",
    "            if verbose:\n",
    "                print(f\"Reading {path}\")\n",
    "\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "            ids, padded_sequences, targets = data[\"id\"], data[\"padded_sequences\"], data[\"target\"]\n",
    "            indices = np.arange(len(ids))\n",
    "            if shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "                ids = ids[indices]\n",
    "                padded_sequences = padded_sequences[indices]\n",
    "                if is_train:\n",
    "                    targets = targets[indices]\n",
    "\n",
    "            for idx in range(len(ids)):\n",
    "                bucket_ids = ids[idx]\n",
    "                bucket = padded_sequences[idx]\n",
    "                if is_train:\n",
    "                    bucket_targets = targets[idx]\n",
    "\n",
    "                for jdx in range(0, len(bucket), batch_size):\n",
    "                    batch_ids = bucket_ids[jdx: jdx + batch_size]\n",
    "                    batch_sequences = bucket[jdx: jdx + batch_size]\n",
    "                    if is_train:\n",
    "                        batch_targets = bucket_targets[jdx: jdx + batch_size]\n",
    "\n",
    "                    if output_format == \"tf\":\n",
    "                        batch_sequences = [batch_sequences[:, i] for i in range(len(features))]\n",
    "\n",
    "                        if is_train:\n",
    "                            yield torch.LongTensor(batch_sequences), batch_targets\n",
    "                        else:\n",
    "                            yield torch.LongTensor(batch_sequences), batch_ids\n",
    "                    else:\n",
    "                        batch_sequences = [torch.LongTensor(batch_sequences[:, i]).to(device) for i in range(len(features))]\n",
    "                        if is_train:\n",
    "                            yield dict(id_=batch_ids,\n",
    "                                       features=torch.LongTensor(batch_sequences),\n",
    "                                       label=torch.LongTensor(batch_targets).to(device),\n",
    "                                       shape=torch.LongTensor(batch_size, batch_size,1))\n",
    "                        else:\n",
    "                            yield dict(id_=batch_ids,\n",
    "                                       features=torch.LongTensor(batch_sequences),\n",
    "                                      shape=torch.LongTensor(batch_size, batch_size,1))\n",
    "        if not is_infinite:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c0c9ac2-b630-42ef-8a62-18bb61352ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###from pytorch_training import train_epoch, eval_model, inference\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(model: torch.nn.Module, optimizer: torch.optim.Optimizer, dataset_train: List[str],\n",
    "                batch_size: int = 64, shuffle: bool = True, print_loss_every_n_batches: int = 500,\n",
    "                device: torch.device = None):\n",
    "    \"\"\"\n",
    "    Делает одну эпоху обучения модели, логируя промежуточные значения функции потерь.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    model: torch.nn.Module\n",
    "        Обучаемая модель.\n",
    "    optimizer: torch.optim.Optimizer\n",
    "        Оптимизатор.\n",
    "    dataset_train: List[str]\n",
    "        Список путей до файлов с предобработанными последовательностями.\n",
    "    batch_size: int, default=64\n",
    "        Размер батча.\n",
    "    shuffle: bool, default=False\n",
    "        Перемешивать ли данные перед подачей в модель.\n",
    "    print_loss_every_n_batches: int, default=500\n",
    "        Число батчей.\n",
    "    device: torch.device, default=None\n",
    "        Девайс, на который переместить данные.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    losses = torch.LongTensor().to(device)\n",
    "    samples_counter = 0\n",
    "    train_generator = batches_generator(dataset_train, batch_size=batch_size, shuffle=shuffle,\n",
    "                                        device=device, is_train=True, output_format=\"torch\")\n",
    "\n",
    "    for num_batch, batch in tqdm_notebook(enumerate(train_generator, start=1), desc=\"Training\"):\n",
    "        output = torch.flatten(model(batch[\"features\"]))\n",
    "        batch_loss = loss_function(output, batch[\"label\"].float())\n",
    "        batch_loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        samples_counter += batch_loss.size(0)\n",
    "\n",
    "        losses = torch.cat([losses, batch_loss], dim=0)\n",
    "        if num_batch % print_loss_every_n_batches == 0:\n",
    "            print(f\"Batches {num_batch - print_loss_every_n_batches + 1} - {num_batch} loss:\"\n",
    "                  f\"{losses[-samples_counter:].mean()}\", end=\"\\r\")\n",
    "            samples_counter = 0\n",
    "\n",
    "    print(f\"Training loss after epoch: {losses.mean()}\", end=\"\\r\")\n",
    "\n",
    "\n",
    "def eval_model(model: torch.nn.Module, dataset_val: List[str], batch_size: int = 32, device: torch.device = None) -> float:\n",
    "    \"\"\"\n",
    "    Скорит выборку моделью и вычисляет метрику ROC AUC.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    model: torch.nn.Module\n",
    "        Модель, которой необходимо проскорить выборку.\n",
    "    dataset_val: List[str]\n",
    "        Список путей до файлов с предобработанными последовательностями.\n",
    "    batch_size: int, default=32\n",
    "        Размер батча.\n",
    "    device: torch.device, default=None\n",
    "        Девайс, на который переместить данные.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    auc: float\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    val_generator = batches_generator(dataset_val, batch_size=batch_size, shuffle=False,\n",
    "                                      device=device, is_train=True, output_format=\"torch\")\n",
    "\n",
    "    for batch in tqdm_notebook(val_generator, desc=\"Evaluating model\"):\n",
    "        targets.extend(batch[\"label\"].detach().cpu().numpy().flatten())\n",
    "        output = model(batch[\"features\"])\n",
    "        preds.extend(output.detach().cpu().numpy().flatten())\n",
    "\n",
    "    return roc_auc_score(targets, preds)\n",
    "\n",
    "\n",
    "def inference(model: torch.nn.Module, dataset_test: List[str], batch_size: int = 32, device: torch.device = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Скорит выборку моделью.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    model: torch.nn.Module\n",
    "        Модель, которой необходимо проскорить выборку.\n",
    "    dataset_test: List[str]\n",
    "        Список путей до файлов с предобработанными последовательностями.\n",
    "    batch_size: int, default=32\n",
    "        Размер батча.\n",
    "    device: torch.device, default=None\n",
    "        Девайс, на который переместить данные.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    scores: pandas.DataFrame\n",
    "        Датафрейм с двумя колонками: \"id\" - идентификатор заявки и \"score\" - скор модели.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    ids = []\n",
    "    test_generator = batches_generator(dataset_test, batch_size=batch_size, shuffle=False,\n",
    "                                       verbose=False, device=device, is_train=False,\n",
    "                                       output_format=\"torch\")\n",
    "\n",
    "    for batch in tqdm_notebook(test_generator, desc=\"Test predictions\"):\n",
    "        ids.extend(batch[\"id_\"])\n",
    "        output = model(batch[\"features\"])\n",
    "        preds.extend(output.detach().cpu().numpy().flatten())\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"id\": ids,\n",
    "        \"score\": preds\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cd2f3ee-12f3-485f-ab4f-c5d0add99c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embed_dim(n_cat: int) -> int:\n",
    "    return min(600, round(1.6 * n_cat**0.56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08f33fcd-fe9a-4620-b1e6-a19e718e9cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pre_since_opened': (20, 9),\n",
       " 'pre_since_confirmed': (18, 8),\n",
       " 'pre_pterm': (18, 8),\n",
       " 'pre_fterm': (17, 8),\n",
       " 'pre_till_pclose': (17, 8),\n",
       " 'pre_till_fclose': (16, 8),\n",
       " 'pre_loans_credit_limit': (20, 9),\n",
       " 'pre_loans_next_pay_summ': (7, 5),\n",
       " 'pre_loans_outstanding': (6, 4),\n",
       " 'pre_loans_total_overdue': (1, 2),\n",
       " 'pre_loans_max_overdue_sum': (4, 3),\n",
       " 'pre_loans_credit_cost_rate': (14, 7),\n",
       " 'pre_loans5': (17, 8),\n",
       " 'pre_loans530': (20, 9),\n",
       " 'pre_loans3060': (10, 6),\n",
       " 'pre_loans6090': (6, 4),\n",
       " 'pre_loans90': (20, 9),\n",
       " 'is_zero_loans5': (2, 2),\n",
       " 'is_zero_loans530': (2, 2),\n",
       " 'is_zero_loans3060': (2, 2),\n",
       " 'is_zero_loans6090': (2, 2),\n",
       " 'is_zero_loans90': (2, 2),\n",
       " 'pre_util': (20, 9),\n",
       " 'pre_over2limit': (20, 9),\n",
       " 'pre_maxover2limit': (20, 9),\n",
       " 'is_zero_util': (2, 2),\n",
       " 'is_zero_over2limit': (2, 2),\n",
       " 'is_zero_maxover2limit': (2, 2),\n",
       " 'enc_paym_0': (4, 3),\n",
       " 'enc_paym_1': (4, 3),\n",
       " 'enc_paym_2': (4, 3),\n",
       " 'enc_paym_3': (4, 3),\n",
       " 'enc_paym_4': (4, 3),\n",
       " 'enc_paym_5': (4, 3),\n",
       " 'enc_paym_6': (4, 3),\n",
       " 'enc_paym_7': (4, 3),\n",
       " 'enc_paym_8': (4, 3),\n",
       " 'enc_paym_9': (4, 3),\n",
       " 'enc_paym_10': (4, 3),\n",
       " 'enc_paym_11': (5, 4),\n",
       " 'enc_paym_12': (4, 3),\n",
       " 'enc_paym_13': (4, 3),\n",
       " 'enc_paym_14': (4, 3),\n",
       " 'enc_paym_15': (4, 3),\n",
       " 'enc_paym_16': (4, 3),\n",
       " 'enc_paym_17': (4, 3),\n",
       " 'enc_paym_18': (4, 3),\n",
       " 'enc_paym_19': (4, 3),\n",
       " 'enc_paym_20': (5, 4),\n",
       " 'enc_paym_21': (4, 3),\n",
       " 'enc_paym_22': (4, 3),\n",
       " 'enc_paym_23': (4, 3),\n",
       " 'enc_paym_24': (5, 4),\n",
       " 'enc_loans_account_holder_type': (7, 5),\n",
       " 'enc_loans_credit_status': (7, 5),\n",
       " 'enc_loans_credit_type': (8, 5),\n",
       " 'enc_loans_account_cur': (4, 3),\n",
       " 'pclose_flag': (2, 2),\n",
       " 'fclose_flag': (2, 2)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_projections = {feat: (max(uniq)+1, compute_embed_dim(max(uniq)+1)) for feat, uniq in uniques.items()}\n",
    "embedding_projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "26ca5c3c-5e41-41f1-bb17-2b204bf7fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditsRNN(nn.Module):\n",
    "    def __init__(self, features, embedding_projections, rnn_units=128, top_classifier_units=32):\n",
    "        super(CreditsRNN, self).__init__()\n",
    "        self._credits_cat_embeddings = nn.ModuleList([self._create_embedding_projection(*embedding_projections[feature]) \n",
    "                                                          for feature in features])\n",
    "                        \n",
    "        self._gru = nn.GRU(input_size=sum([embedding_projections[x][1] for x in features]),\n",
    "                             hidden_size=rnn_units, batch_first=True, bidirectional=False)\n",
    "        self._hidden_size = rnn_units\n",
    "        self._top_classifier = nn.Linear(in_features=rnn_units, out_features=top_classifier_units)\n",
    "        self._intermediate_activation = nn.ReLU()\n",
    "        self._head = nn.Linear(in_features=top_classifier_units, out_features=1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        batch_size = features[0].shape[0]\n",
    "        embeddings = [embedding(features[i]) for i, embedding in enumerate(self._credits_cat_embeddings)]\n",
    "        concated_embeddings = torch.cat(embeddings, dim=-1)\n",
    "        \n",
    "        _, last_hidden = self._gru(concated_embeddings)\n",
    "        last_hidden = torch.reshape(last_hidden.permute(1, 2, 0), shape=(batch_size, self._hidden_size))\n",
    "                                \n",
    "        classification_hidden = self._top_classifier(last_hidden)\n",
    "        activation = self._intermediate_activation(classification_hidden)\n",
    "        raw_output = self._head(activation)\n",
    "        return raw_output\n",
    "    \n",
    "    @classmethod\n",
    "    def _create_embedding_projection(cls, cardinality, embed_size, add_missing=True, padding_idx=0):\n",
    "        add_missing = 1 if add_missing else 0\n",
    "        return nn.Embedding(num_embeddings=cardinality+add_missing, embedding_dim=embed_size, padding_idx=padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12584743-be3a-4082-9009-b983a25fc102",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoints = \"/trinity/home/team08/workspace/main_project/checkpoints/\"\n",
    "es = EarlyStopping(patience=3, mode=\"max\", verbose=True, save_path=os.path.join(path_to_checkpoints, \"best_checkpoint.pt\"), \n",
    "                   metric_name=\"ROC-AUC\", save_format=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7338aaf3-0ffa-4111-88af-093f645c81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_batch_size = 128\n",
    "val_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e2eee47-5510-4143-aaf8-242dc8c1fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CreditsRNN(features, embedding_projections).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3bf2f729-6ca5-41e2-ac2c-1b74185dba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=1e-3, params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec34e47f-83e2-4de9-88b9-30d4519b33db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreditsRNN(\n",
       "  (_credits_cat_embeddings): ModuleList(\n",
       "    (0): Embedding(21, 9, padding_idx=0)\n",
       "    (1-2): 2 x Embedding(19, 8, padding_idx=0)\n",
       "    (3-4): 2 x Embedding(18, 8, padding_idx=0)\n",
       "    (5): Embedding(17, 8, padding_idx=0)\n",
       "    (6): Embedding(21, 9, padding_idx=0)\n",
       "    (7): Embedding(8, 5, padding_idx=0)\n",
       "    (8): Embedding(7, 4, padding_idx=0)\n",
       "    (9): Embedding(2, 2, padding_idx=0)\n",
       "    (10): Embedding(5, 3, padding_idx=0)\n",
       "    (11): Embedding(15, 7, padding_idx=0)\n",
       "    (12): Embedding(18, 8, padding_idx=0)\n",
       "    (13): Embedding(21, 9, padding_idx=0)\n",
       "    (14): Embedding(11, 6, padding_idx=0)\n",
       "    (15): Embedding(7, 4, padding_idx=0)\n",
       "    (16): Embedding(21, 9, padding_idx=0)\n",
       "    (17-21): 5 x Embedding(3, 2, padding_idx=0)\n",
       "    (22-24): 3 x Embedding(21, 9, padding_idx=0)\n",
       "    (25-27): 3 x Embedding(3, 2, padding_idx=0)\n",
       "    (28-38): 11 x Embedding(5, 3, padding_idx=0)\n",
       "    (39): Embedding(6, 4, padding_idx=0)\n",
       "    (40-47): 8 x Embedding(5, 3, padding_idx=0)\n",
       "    (48): Embedding(6, 4, padding_idx=0)\n",
       "    (49-51): 3 x Embedding(5, 3, padding_idx=0)\n",
       "    (52): Embedding(6, 4, padding_idx=0)\n",
       "    (53-54): 2 x Embedding(8, 5, padding_idx=0)\n",
       "    (55): Embedding(9, 5, padding_idx=0)\n",
       "    (56): Embedding(5, 3, padding_idx=0)\n",
       "    (57-58): 2 x Embedding(3, 2, padding_idx=0)\n",
       "  )\n",
       "  (_gru): GRU(258, 128, batch_first=True)\n",
       "  (_top_classifier): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (_intermediate_activation): ReLU()\n",
       "  (_head): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04b4ca9b-e959-4169-8835-ecfddbba8635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3967394/1341478050.py:47: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for num_batch, batch in tqdm_notebook(enumerate(train_generator, start=1), desc=\"Training\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84296bbd28bb45b69ca7d5789ffcfef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnpicklingError",
     "evalue": "A load persistent id instruction was encountered,\nbut no persistent_load function was specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "Cell \u001b[0;32mIn[69], line 47\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, dataset_train, batch_size, shuffle, print_loss_every_n_batches, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m samples_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     44\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m batches_generator(dataset_train, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m     45\u001b[0m                                     device\u001b[38;5;241m=\u001b[39mdevice, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_batch, batch \u001b[38;5;129;01min\u001b[39;00m tqdm_notebook(\u001b[38;5;28menumerate\u001b[39m(train_generator, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     48\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(model(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     49\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m loss_function(output, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[68], line 57\u001b[0m, in \u001b[0;36mbatches_generator\u001b[0;34m(list_of_paths, batch_size, shuffle, is_infinite, verbose, device, output_format, is_train)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 57\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m ids, padded_sequences, targets \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadded_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     60\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(ids))\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: A load persistent id instruction was encountered,\nbut no persistent_load function was specified."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch {epoch+1}\")\n",
    "    train_epoch(model, optimizer, dataset_train, batch_size=train_batch_size, \n",
    "                shuffle=True, print_loss_every_n_batches=500, device=device)\n",
    "    \n",
    "    val_roc_auc = eval_model(model, dataset_val, batch_size=val_batch_size, device=device)\n",
    "    es(val_roc_auc, model)\n",
    "    \n",
    "    if es.early_stop:\n",
    "        print(\"Early stopping reached. Stop training...\")\n",
    "        break\n",
    "    torch.save(model.state_dict(), os.path.join(path_to_checkpoints, f\"epoch_{epoch+1}_val_{val_roc_auc:.3f}.pt\"))\n",
    "    \n",
    "    train_roc_auc = eval_model(model, dataset_train, batch_size=val_batch_size, device=device)\n",
    "    print(f\"Epoch {epoch+1} completed. Train ROC AUC: {train_roc_auc}, val ROC AUC: {val_roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
